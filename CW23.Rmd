---
title: "MA40198 Coursework 2023"
author: "Your names here"

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 [4 marks]

Consider the following observed sample: 

```{r}
#| code-fold: show
y_sample_q1 <- scan("http://people.bath.ac.uk/kai21/ASI/CW_2023/y_sample_q1.txt")
```

Plot 40 contours of the negative loglikelihood function of the parameter $\boldsymbol{\lambda}$ over the region defined by $-\pi/2<\lambda_1<\pi/2$ and $0<\lambda_2<50$. The contours should be sufficiently smooth and cover the entire region. You should indicate a smaller region delimited by a contour that contains the global minimum.


## Solution to Question 1

We use a reparameterisation of $\boldsymbol{\lambda} = \boldsymbol{\theta}$ with $\lambda_1 = tan^{-1}(\theta_1)$ and $\lambda_2 = e^{\theta_2}$

```{r}
log_dens_inside_sum <- expression(
  - log(1 + (y / (exp(theta2) +  2 * j)) ^ 2)
)

log_dens_outside_sum <- expression(
  + atan(theta1) * y 
  + exp(theta2) * log(cos(atan(theta1)))
  + log(2) * (exp(theta2) - 2)
  + 2 * lgamma(exp(theta2) / 2)
  - log(pi)
  - lgamma(exp(theta2))
)

deriv_inside_sum <- deriv(log_dens_inside_sum, c("theta2"), function.arg = c("theta2", "y", "j"))
deriv_outside_sum <- deriv(log_dens_outside_sum, c("theta1", "theta2"), function.arg = c("theta1", "theta2", "y"))

nll_fn <- function(theta, y, N = 10000) {
  inside_sum <- NA
  
  for(j in 0:N) {
    res <- deriv_inside_sum(theta2 = theta[2], y = y, j)
    inside_sum[j + 1] <- sum(as.numeric(res))
  }
  
  g_sum <- sum(inside_sum)
  
  res <- deriv_outside_sum(theta1 = theta[1], theta2 = theta[2], y = y)
  
  fn <- sum(as.numeric(res)) + g_sum
  
  - fn
}

lambda1 <- seq(-pi / 2, pi / 2, length = 102)[-c(1, 102)]
lambda2 <- seq(0, 50, length = 102)[-c(1, 102)]

theta1 <- tan(lambda1)
theta2 <- log(lambda2)

M <- matrix(NA, nrow = 100, ncol = 100)

for (i in seq_along(theta1)){
  for (j in seq_along(theta2)){
    # 100 for test runs, 10000 when finalised
    M[i,j] <- nll_fn(theta = c(theta1[i], theta2[j]), y = y_sample_q1, N = 100)
  }
}

levs <- seq(min(M, na.rm = T), max(M, na.rm = T), length = 40) / 16

contour(
  x = lambda1,
  y = lambda2,
  z = M,
  levels = levs,
  xlab = expression(lambda[1]),
  ylab = expression(lambda[2])
)
```


## Question 2 [6 marks]

Find the maximum likelihood estimate $\widehat{\lambda}=(\hat{\lambda}_1,\hat{\lambda}_2)^T$ by 
picking the best out of 100 optimisations (using the BFGS algorithm) where each optimisation uses a different initial value. The following data frame gives the list of  initial values to be used.

```{r}
#| code-fold: show
L0 <- read.table("http://people.bath.ac.uk/kai21/ASI/CW_2023/starting_vals_q2.txt")
```

## Solution to Question 2

```{r}
grad_fn <- function(theta, y, N = 10000) {
  inside_sum <- NA
  
  for(j in 0:N) {
    res <- deriv_inside_sum(theta2 = theta[2], y = y, j)
    inside_sum[j + 1] <- sum(attr(res,"gradient"))
  }
  
  g_sum <- sum(inside_sum, na.rm = T)
  
  res <- deriv_outside_sum(theta1 = theta[1], theta2 = theta[2], y = y)
  
  grad <- apply(attr(res,"gradient"), 2, sum) + c(0, g_sum)
  
  - grad
}

reparam <- function(lambda) {
  c(tan(lambda[1]), log(lambda[2]))
}

check_fail <- function(fit) {
  no_convergence <- fit$convergence > 0

  no_variance <- inherits(try(solve(fit$hessian)), "try-error")

  null_variance <- F
  NA_variance <- F

  if (!no_variance) {
    # checks if asymptotic variance are NaN
    NA_variance <- as.logical(sum(is.nan(diag(solve(fit$hessian)))))

    if(!NA_variance){
      # checks if asymptotic variance are zero up to machine precision
      null_variance <- as.logical(sum(diag(solve(fit$hessian)) < .Machine$double.eps ^ 0.5))
    }
  }

  fail <- no_variance | no_convergence | NA_variance | null_variance

  return(fail)
}

fit_for_N <- function(N) {
  fit <- NULL

  for(i in 1:NROW(L0)) {
    fit[[i]] <- 
      optim(
        par = reparam(as.numeric(L0[i, ])),
        fn = nll_fn,
        gr = grad_fn,
        method = 'BFGS',
        y = y_sample_q1,
        N = N,
        hessian = T
      )
    
    if(check_fail(fit[[i]])) {
      fit[[i]]$value <- NULL
    }
  }
  
  nll_vals <- lapply(fit, \(x) as.numeric(x$value))
  
  fit[[which.min(nll_vals)]]
}

fit <- fit_for_N(100)

mle_lambda1 <- atan(fit$par[1])
mle_lambda2 <- exp(fit$par[2])
```



## Question 3 [4 marks]

Check the sensitivity of the MLE to the choice of $N$ by plotting (separately) the values of $\hat{\lambda}_1$ and $\hat{\lambda}_2$ as function of $\log_{10}(N)$. You should use the values $10^1,10^2, 10^3,10^4,10^5,10^6$ for $N$. What conclusions can you make from these two plots?

## Solution to Question 3

```{r}
reverse_reparam <- function(theta) {
  c(atan(theta[1]), exp(theta[2]))
}

N_vals <- 10 ^ seq(1, 3)

fit <- sapply(N_vals, \(x) reverse_reparam(fit_for_N(x)$par))
```


## Question 4 [4 marks]

Compute the maximum likelihood estimate of the mean parameter
$$\mu(\boldsymbol{\lambda}_*)=E[Y|\boldsymbol{\lambda}_*]=\int_{\mathcal R} y\,f(y|\boldsymbol{\lambda}_*)dy\,.$$
Also compute an asymptotic 95% confidence interval for $\mu(\boldsymbol{\lambda}_*)$. State clearly any assumptions you have made.

*A5* - changing integral and derivative

We define 
$$g_1(\boldsymbol{\theta}) = \mu(\boldsymbol{\theta}) = \int_{\mathcal R} y\,f(y|\boldsymbol{\theta})dy$$

etc...

$$g_1(\boldsymbol{\theta}) = \int_{\mathcal R} y\,\exp(\log f(y|\boldsymbol{\theta}))dy$$
This is helpful since we can use our existing functions to calculate $\log f(y|\boldsymbol{\theta})$ and R's **integrate()** function.

$$\frac{\partial}{\partial\theta_i} g_1(\boldsymbol{\theta}) = \int_{\mathcal R} yf(y|\boldsymbol{\theta})\frac{\partial}{\partial\theta_i}\log f(y|\boldsymbol{\theta})dy\,\quad i =1,2$$

## Solution to Question 4

```{r}
mu_integrand <- function(y, lambda1, lambda2, N) {
  sapply(y, \(x) x * exp(-nll_fn(theta = reparam(c(lambda1, lambda2)), x, N)))
}

est <- integrate(mu_integrand, lower = -30, upper = 30, lambda1 = atan(best_fit$par[1]), lambda2 = exp(best_fit$par[2]), N = 10000)$value

J_integrand <- function(y, lambda1, lambda2, N, comp = 1) {
  theta <- reparam(c(lambda1, lambda2))
  
  res <- sapply(y, \(x) x * exp(-nll_fn(theta, x, N)) * - grad_fn(theta, x, N))
  
  res[comp, ]
}

J <- sapply(
  c(1:2), \(x) integrate(J_integrand, lower = -30, upper = 30, lambda1 = atan(best_fit$par[1]), lambda2 = exp(best_fit$par[2]), N = 10000, comp = x)$value
)

se <- as.numeric(sqrt(t(J) %*% solve(best_fit$hess) %*% J))

est + c(-1, 1) * se
```

```{r}  
xx <- seq(-5,30,length=100)
yy_dens <- rep(NA,100)

for (i in 1:100){
  yy_dens[i] <- log_dens(xx[i], lambda1, lambda2, 100)
}

plot(xx,yy_dens,type="l")
```



## Question 5 [4 marks]

Compute an asymptotic 95% confidence interval for the unknown parameter $\lambda^*_2$ using:

* the asymptotic normal approximation to the distribution $\hat{\lambda}_2$

* the asymptotic normal approximation to the distribution $\log( \hat{\lambda}_2)$



## Solution to Question 5

```{r}
# your code here
```



## Question 6 [4 marks]

Use the generalised likelihood ratio to test the hypotheses:

$$H_0:\,\mu(\boldsymbol{\lambda}_*)=5\qquad \mbox{vs}\qquad H_a:\,\mu(\boldsymbol{\lambda}_*)\neq 5$$

using a significance level $\alpha=0.05$.

Separately, also test 

$$H_0:\,\lambda^*_2=5\qquad \mbox{vs}\qquad H_a:\,\lambda^*_2\neq 5$$

using a significance level $\alpha=0.05$.

## Solution to Question 6

```{r}
# your code here
```


## Question 7 [10 marks]

Consider the following  data frame

```{r}
#| code-fold: show
data_q7 <-read.table("http://people.bath.ac.uk/kai21/ASI/CW_2023/data_q7.txt")

```
that contains a bivariate sample 
$$(x_1,y_1),\,(x_2,y_2),\,\ldots,\,(x_n,y_n)$$
of size $n=300$.




Use the parametric family $\mathcal F_1$ defined in Question 1 to find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$, that is $f_*(y|x)$. 
The model should be defined by  specifying the mean function $\mu(\boldsymbol{\theta}^{(1)},x)$ as follows:

$$
\mu(\boldsymbol{\theta}^{(1)},x) =g^{-1}(\theta_1+\theta_2\,x +\theta_3\,x^2+\theta_4\,x^3 +\cdots+\theta_{p+1}\,x^p)
$$

for some choice of link function $g$ and some choice of integer $p\geq 1$.


From a set of candidate models (that is for different choices of $g$ and $p$),  choose the model with the smallest AIC (Akaike Information Criterion). Only present the results from the maximum likelihood estimation from the best chosen model and simply comment on the other models considered.

Now, repeat the same process  above to find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$ but now based on the Gamma parametric family:

$$
\mathcal F_{gamma}=\left\{f(y|\lambda_1,\lambda_2)=\frac{\lambda_2^{\lambda_1}}{\Gamma(\lambda_1)}y^{\lambda_1-1}\exp(-\lambda_2\,y)\,:\, \lambda_1>0\,,\lambda_2>0,y>0\right\}
$$

Finally, find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$
but now based on the Normal parametric family:

$$
\mathcal F_{normal}=\left\{f(y|\lambda_1,\lambda_2)=\frac{1}{\lambda_2\sqrt{2\pi}}\,\exp\left(-\frac{(y-\lambda_1)^2}{2\lambda_2^2}\right)\,:\, \lambda_1\in {\mathcal R},\,\lambda_2>0,y\in {\mathcal R}\right\}
$$


For each of the three chosen models, you should plot the data together with the maximum likelihood estimate of the mean function as well as corresponding asymptotic 95\% confidence bands in the range $x\in(-3,3)$. Comment on the differences between the confidence bands and the mean function estimates. You must select the best model out of the three, based on the Akaike  Information Criterion. 


## Solution to Question 7

```{r}
# your code here
```


## Question 8 [4 marks]

Use the data in Question 7  to compute 95\% confidence intervals for the least worse value of the mean function  at each $x$, that is $\mu(\boldsymbol{\theta}^{(1)}_\dagger,x)$
for each of the three parametric families: $\mathcal F_1$, the Gamma and the Normal. Plot the computed confidence bands in the range $x\in(-3,3)$ for each parametric family and comment on the differences obtained.


## Solution to Question 8

```{r}
# your code here
```