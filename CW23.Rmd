---
title: "MA40198 Coursework 2023"
author: "Sarah Larkin and Louis Bennett"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# suppress warnings
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE, fig.align = 'center')
```

## Question 1 [4 marks]

Consider the following observed sample:

```{r, cache=TRUE}
#| code-fold: show
y_sample_q1 <- scan("https://people.bath.ac.uk/kai21/ASI/CW_2023/y_sample_q1.txt")
```

Plot 40 contours of the negative loglikelihood function of the parameter $\boldsymbol{\lambda}$ over the region defined by $-\pi/2<\lambda_1<\pi/2$ and $0<\lambda_2<50$. The contours should be sufficiently smooth and cover the entire region. You should indicate a smaller region delimited by a contour that contains the global minimum.

## Solution to Question 1

We use a reparameterisation $\boldsymbol{\lambda} = (\lambda_1,\lambda_2)^T = (tan^{-1}(\theta_1),e^{\theta_2})^T$.

```{r}
log_dens_inside_sum <- expression(
  - log(1 + (y / (exp(theta2) +  2 * j)) ^ 2)
)

log_dens_outside_sum <- expression(
  + atan(theta1) * y 
  + exp(theta2) * log(cos(atan(theta1)))
  + log(2) * (exp(theta2) - 2)
  + 2 * lgamma(exp(theta2) / 2)
  - log(pi)
  - lgamma(exp(theta2))
)

deriv_inside_sum <- deriv(log_dens_inside_sum, c("theta2"), function.arg = c("theta2", "y", "j"))
deriv_outside_sum <- deriv(log_dens_outside_sum, c("theta1", "theta2"), function.arg = c("theta1", "theta2", "y"))

nll_fn <- function(theta, y, N = 10000) {
  inside_sum <- NA
  
  for(j in 0:N) {
    res <- deriv_inside_sum(theta2 = theta[2], y = y, j)
    inside_sum[j + 1] <- sum(as.numeric(res))
  }
  
  g_sum <- sum(inside_sum)
  
  res <- deriv_outside_sum(theta1 = theta[1], theta2 = theta[2], y = y)
  
  fn <- sum(as.numeric(res)) + g_sum
  
  - fn
}

grad_fn <- function(theta, y, N = 10000) {
  inside_sum <- NA
  
  for(j in 0:N) {
    res <- deriv_inside_sum(theta2 = theta[2], y = y, j)
    inside_sum[j + 1] <- sum(attr(res,"gradient"))
  }
  
  g_sum <- sum(inside_sum, na.rm = T)
  
  res <- deriv_outside_sum(theta1 = theta[1], theta2 = theta[2], y = y)
  
  grad <- apply(attr(res,"gradient"), 2, sum) + c(0, g_sum)
  
  - grad
}
```

We plot the contour of the negative log likelihood, $\phi(\boldsymbol{\lambda}|y)$ here:

```{r, fig.height=6, fig.align='center', cache=TRUE, eval=TRUE}
N_val <- 10000

lambda1 <- seq(-pi / 2, pi / 2, length = 102)[-c(1, 102)]
lambda2 <- seq(0, 50, length = 102)[-c(1, 102)]

theta1 <- tan(lambda1)
theta2 <- log(lambda2)

M <- matrix(NA, nrow = 100, ncol = 100)

for (i in seq_along(theta1)){
  for (j in seq_along(theta2)){
    M[i,j] <- nll_fn(theta = c(theta1[i], theta2[j]), y = y_sample_q1, N = N_val)
  }
}

# Divide by 8 to get smaller region that contains the global minimum
levs <- seq(min(M, na.rm = T), max(M, na.rm = T), length = 40) / 8

contour(
  x = lambda1,
  y = lambda2,
  z = M,
  levels = levs,
  drawlabels = F,
  xlab = expression(lambda[1]),
  ylab = expression(lambda[2]),
  main = expression(bold("Contours of \u03D5(\u03BB|y)"))
)
```

## Question 2 [6 marks]

Find the maximum likelihood estimate $\widehat{\lambda}=(\hat{\lambda}_1,\hat{\lambda}_2)^T$ by picking the best out of 100 optimisations (using the BFGS algorithm) where each optimisation uses a different initial value. The following data frame gives the list of initial values to be used.

```{r, cache=TRUE}
#| code-fold: show
L0 <- read.table("https://people.bath.ac.uk/kai21/ASI/CW_2023/starting_vals_q2.txt")
```

## Solution to Question 2

We run optim over each starting value in the dataframe and pick the MLE, $\boldsymbol{\widehat{\lambda}}$, with the lowest value of $\phi(\boldsymbol{\widehat{\lambda}})$.

```{r, cache=TRUE}
reparam <- function(lambda) c(tan(lambda[1]), log(lambda[2]))

check_fail <- function(fit) {
  no_convergence <- fit$convergence > 0

  no_variance <- inherits(try(solve(fit$hessian)), "try-error")

  null_variance <- F
  NA_variance <- F

  if (!no_variance) {
    # checks if asymptotic variance are NaN
    NA_variance <- as.logical(sum(is.nan(diag(solve(fit$hessian)))))

    if(!NA_variance){
      # checks if asymptotic variance are zero up to machine precision
      null_variance <- as.logical(sum(diag(solve(fit$hessian)) < .Machine$double.eps ^ 0.5))
    }
  }

  fail <- no_variance | no_convergence | NA_variance | null_variance

  return(fail)
}

pick_optim <- function(fn = nll_fn, gr = grad_fn, starting_vals, ...) {
  fit <- NULL
  
  for(i in 1:NROW(starting_vals)) {
    fit[[i]] <- tryCatch(
      expr = optim(
        par = starting_vals[i, ],
        fn = fn,
        gr = gr,
        method = 'BFGS',
        ...,
        hessian = T
      ),
      error = function(e) list(value = NULL, hessian = diag(length(starting_vals[i, ])), convergence = 0)
    )
      
    if(check_fail(fit[[i]])) {
      fit[[i]]$value <- NULL
    }
  }
  
  nll_vals <- lapply(fit, \(x) as.numeric(x$value))
  
  # pick optimisation that minimises phi
  fit[[which.min(nll_vals)]]
}

L0_starting_vals <- matrix(NA, nrow = NROW(L0), ncol = 2)

# reparametrise starting values
for(i in 1:NROW(L0)) {
  L0_starting_vals[i, ] <- reparam(as.numeric(L0[i, ]))
}

fit_q2 <- pick_optim(starting_vals = L0_starting_vals, N = N_val, y = y_sample_q1)

fit_q2
```

We undo the reparameterisation to calculate $\boldsymbol{\widehat\lambda}$:

```{r,cache=TRUE}
reverse_reparam <- function(theta) c(atan(theta[1]), exp(theta[2]))

reverse_reparam(fit_q2$par)
```


## Question 3 [4 marks]

Check the sensitivity of the MLE to the choice of $N$ by plotting (separately) the values of $\hat{\lambda}_1$ and $\hat{\lambda}_2$ as function of $\log_{10}(N)$. You should use the values $10^1,10^2, 10^3,10^4,10^5,10^6$ for $N$. What conclusions can you make from these two plots?

## Solution to Question 3

```{r, cache=TRUE}
N_vals <- 10 ^ seq(1, 6)

# Apply over the N values
fit_q3 <- lapply(
  N_vals,
  \(x) {
    optim(
      par = fit_q2$par,
      fn = nll_fn,
      gr = grad_fn,
      method = 'BFGS',
      N = x,
      y = y_sample_q1
    )$par
  }
)

# undo reparam
lambda1_vals <- sapply(fit_q3, \(x) reverse_reparam(x)[1])
lambda2_vals <- sapply(fit_q3, \(x) reverse_reparam(x)[2])
```

We now plot the data:

```{r, fig.width=8}
par(mfrow = c(1, 2))

plot(
  x = log(N_vals, 10),
  y = lambda1_vals,
  type = 'l',
  xlab = "log(N)",
  ylab = expression(lambda[1])
)

plot(
  x = log(N_vals, 10),
  y = lambda2_vals,
  type = 'l',
  xlab = "log(N)",
  ylab = expression(lambda[2])
)

title(expression(bold(paste("Sensitivity of ", hat("\u03BB"), " to N"))), line = -2, outer = TRUE)
```

We can see that $\boldsymbol{\hat\lambda}$ is very sensitive to $N$ at smaller values (up to $N^3$) but then becomes very consistent as $N$ grows larger, and actually doesn't really change at all up to $N^6$.

## Question 4 [4 marks]

Compute the maximum likelihood estimate of the mean parameter
$$\mu(\boldsymbol{\lambda}_*)=E[Y|\boldsymbol{\lambda}_*]=\int_{\mathbb R} y\,f(y|\boldsymbol{\lambda}_*)dy\,.$$

Also compute an asymptotic 95% confidence interval for $\mu(\boldsymbol{\lambda}_*)$. State clearly any assumptions you have made.

## Solution to Question 4

We define 
$$g_1(\boldsymbol{\lambda}) = \mu(\boldsymbol{\lambda}) = \int_{\mathbb R} y\,f(y|\boldsymbol{\lambda})dy$$

We can use our existing NLL function to calculate $\log f(y|\boldsymbol{\lambda})$ and then use R's **integrate()** function.

We first check that our density integrates to 1 over $\mathbb R$.

```{r, cache=TRUE}
integrand <- function(y, lambda1, lambda2, N) sapply(y, \(x) exp(-nll_fn(theta = reparam(c(lambda1, lambda2)), x, N)))

integrate(integrand, lower = -Inf, upper = Inf, lambda1 = atan(fit_q2$par[1]), lambda2 = exp(fit_q2$par[2]), N = N_val)
```

Now we notice that
$$1 = \int_{\mathbb R} f(y|\boldsymbol{\lambda})dy = e^{\lambda_2 \log(\cos \lambda_1)} \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy$$

So,
$$\tag{1} -\lambda_2 \log(\cos \lambda_1) = \log \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy$$

Taking $\frac{\partial}{\partial \lambda_1}$ of the RHS of $(1)$ and using the chain rule we get:
$$\tag{2} \frac{\partial}{\partial \lambda_1} \log \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy = \frac {\frac{\partial}{\partial \lambda_1} \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy} {\int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy} = \frac {\frac{\partial}{\partial \lambda_1} \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy} {\int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy} = \frac {e^{\lambda_2 \log(\cos \lambda_1)} \frac{\partial}{\partial \lambda_1} \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy} {e^{\lambda_2 \log(\cos \lambda_1)} \int_{\mathbb R} e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy}$$

The denominator of the fraction on the RHS of $(2)$ is exactly $\int_{\mathbb R} f(y|\boldsymbol{\theta})dy$ and using assumption *A5* from the lecture notes to interchange the derivative and integral we see the numerator is simply:
$$\int_{\mathbb R} e^{\lambda_2 \log(\cos \lambda_1)}\frac{\partial}{\partial \lambda_1}(e^{\lambda_1 y})\,g(y|\boldsymbol{\lambda})dy = \int_{\mathbb R} ye^{\lambda_2 \log(\cos \lambda_1)}e^{\lambda_1 y}g(y|\boldsymbol{\lambda})dy = \int_{\mathbb R} y\,f(y|\boldsymbol{\lambda})dy = \mu(\boldsymbol{\lambda})$$

Applying $\frac{\partial}{\partial \lambda_1}$ to the LHS of $(1)$ we finally get that 
$$\tag{*} \mu(\boldsymbol{\lambda}) = \lambda_2 \tan \lambda_1 = e^{\theta_2} \theta_1$$

We evaluate this expression at $\hat\theta$:

```{r, cache=TRUE}
expr_mu <- expression(exp(theta2) * theta1)

deriv_pack_mu <- deriv(expr_mu, c("theta1", "theta2"), function.arg = c("theta1", "theta2"), hessian = T)

res <- deriv_pack_mu(fit_q2$par[1], fit_q2$par[2])

est <- as.numeric(res)

est
```

And check our answer with an integral:

```{r, cache=TRUE}
mu_integrand <- function(y, lambda1, lambda2, N) sapply(y, \(x) x * exp(-nll_fn(theta = reparam(c(lambda1, lambda2)), x, N)))

est_int <- integrate(mu_integrand, lower = -100, upper = 100, lambda1 = atan(fit_q2$par[1]), lambda2 = exp(fit_q2$par[2]), N = N_val)$value

est_int
```

These results are likely slightly different due to us using `r N_val` as an approximation to an infinite sum.

Using results from lectures about confidence intervals and the Delta Method, we compute an asymptotic 95% CI for this estimate:

```{r, cache=TRUE}
hess_q4 <- matrix(as.numeric(attr(res, 'hessian')), nrow = 2, ncol = 2)

J <- as.numeric(attr(res, 'gradient'))
  
se <- as.numeric(sqrt(t(J) %*% solve(fit_q2$hess) %*% J))

est + c(-1.96, 1.96) * se
```

We can also use the original expression for $\mu(\boldsymbol\lambda)$, noticing that
$$\frac{\partial}{\partial\lambda_i} g_1(\boldsymbol{\lambda}) = \int_{\mathbb R} yf(y|\boldsymbol{\lambda})\frac{\partial}{\partial\lambda_i}\log f(y|\boldsymbol{\lambda})dy\,\quad i =1,2$$

```{r, cache=TRUE}
J_integrand <- function(y, lambda1, lambda2, N, comp = 1) {
  theta <- reparam(c(lambda1, lambda2))
  
  res <- sapply(y, \(x) x * exp(-nll_fn(theta, x, N)) * - grad_fn(theta, x, N))
  
  res[comp, ]
}

# Apply the integration from -30 to 30 as this covers majority of distribution
J_int <- sapply(
  c(1:2), \(x) integrate(J_integrand, lower = -30, upper = 30, lambda1 = atan(fit_q2$par[1]), lambda2 = exp(fit_q2$par[2]), N = N_val, comp = x)$value
)

se_int <- as.numeric(sqrt(t(J_int) %*% solve(fit_q2$hess) %*% J_int))

est_int + c(-1.96, 1.96) * se_int
```

## Question 5 [4 marks]

Compute an asymptotic 95% confidence interval for the unknown parameter $\lambda^*_2$ using:

- the asymptotic normal approximation to the distribution $\hat{\lambda}_2$

- the asymptotic normal approximation to the distribution $\log( \hat{\lambda}_2)$

## Solution to Question 5

First we compute an asymptotic 95% confidence interval using $\hat{\lambda}_2$. By results in lectures (under Assumptions *A1* - *A5*):
$$\widetilde{\boldsymbol{\mathcal I}(\boldsymbol{\theta}^*)}^{1/2}(\widehat{\boldsymbol{\theta}}_n(\boldsymbol{\mathcal Y})-\boldsymbol{\theta}^*)\stackrel{d}{\to} N(\boldsymbol{0}_{p+m},\boldsymbol{I}_{p+m})$$

Where $\widetilde{\boldsymbol{\mathcal I}(\boldsymbol{\theta}^*)}^{1/2}$ is our estimator to the variance, as defined in lectures.

```{r}
asymp_var <- solve(fit_q2$hessian)
# Take the 2,2 component to get confidence interval for lambda2
exp(fit_q2$par[2]) + c(-1.96, 1.96) * sqrt(asymp_var[2,2])
```

We now compute a confidence interval for $g_1(\boldsymbol\lambda) = \log({\lambda}_2) = \theta_2$ using the Delta Method. From lecture notes, we can estimate the variance by $\boldsymbol{J}_{\boldsymbol{g}}(\widehat{\boldsymbol{\theta}})[\nabla^2_{\! \boldsymbol{\theta}}\phi(\widehat{\boldsymbol{\theta}}|\boldsymbol{y})]^{-1}\boldsymbol{J}^T_{\boldsymbol{g}}(\widehat{\boldsymbol{\theta}})$, where $\boldsymbol{J}_{\boldsymbol{g}}(\widehat{\boldsymbol{\theta}})$ is defined as in the notes.

```{r}
J_g <- reverse_reparam(grad_fn(theta = fit_q2$par, y_sample_q1, N_val))
# Again, taking 2,2 component to get confidence interval for lambda2
se <- sqrt(J_g[2] * asymp_var[2,2] * J_g[2])
CI <- fit_q2$par[2] + c(-1.96, 1.96) * se
```

Then using invariance of the MLE we can get a confidence interval for $\lambda^*_2$:

```{r}
exp(CI)
```

## Question 6 [4 marks]

Use the generalised likelihood ratio to test the hypotheses:

$$H_0:\,\mu(\boldsymbol{\lambda}_*)=5\qquad \mbox{vs}\qquad H_a:\,\mu(\boldsymbol{\lambda}_*)\neq 5$$

using a significance level $\alpha=0.05$.

Separately, also test

$$H_0:\,\lambda^*_2=5\qquad \mbox{vs}\qquad H_a:\,\lambda^*_2\neq 5$$

using a significance level $\alpha=0.05$.

## Solution to Question 6

Using $(*)$ from Q4, we code expressions for given $H_0: \mu(\boldsymbol{\lambda}_*)=\lambda_2\tan(\lambda_1)=5$, i.e. $\theta_1=tan(\lambda_1)=\frac{5}{\lambda_2}=\frac{5}{e^{\theta_2}}$.

```{r, cache=TRUE}
# Reparametrisation of lambda1 = atan(theta) as before
log_dens_inside_sum_q6 <- expression(
  - log(1 + (y/((5/theta1)+2*j))^2)
)

log_dens_outside_sum_q6 <- expression(
  + atan(theta1)*y
  + (5/theta1)*log(cos(atan(theta1)))
  + ((5/theta1)-2)*log(2)
  + 2*lgamma(0.5 * (5/theta1))
  - log(pi)
  - lgamma(5/theta1)
)

deriv_outside_sum_q6 <- deriv(log_dens_outside_sum_q6, c("theta1"), function.arg = c("theta1", "y"))
deriv_inside_sum_q6 <- deriv(log_dens_inside_sum_q6, c("theta1"), function.arg = c("theta1", "y", "j"))

nll_fn_q6_p1 <- function(theta1, y, N = 10000) {
  inside_sum <- NA
  
  for (j in 0:N) {
    res <- deriv_inside_sum_q6(theta1, y, j)
    inside_sum[j+1] <- sum(as.numeric(res))
  }
  
  res <- deriv_outside_sum_q6(theta1, y)
  
  fn <- sum(as.numeric(res)) + sum(inside_sum)
  
  - fn
}

grad_fn_q6_p1 <- function(theta1, y, N = 10000) {
  inside_sum <- NA
  
  for (j in 0:N) {
    res2 <- deriv_inside_sum_q6(theta1, y, j)
    inside_sum[j + 1] <- apply(attr(res2,"gradient"), 2, sum)  
  }
  
  res <- deriv_outside_sum_q6(theta1, y)
  
  grad <- apply(attr(res,"gradient"), 2, sum) + sum(inside_sum)  
  
  - grad
}

fit_q6_p1 <- pick_optim(fn = nll_fn_q6_p1, gr = grad_fn_q6_p1, starting_vals = matrix(tan(L0$lambda1)), N = N_val, y = y_sample_q1)

# test statistic from lectures
glrt <- 2 * (-fit_q2$value + fit_q6_p1$value)
glrt > qchisq(0.95, 1)
```

So we reject $H_0$ in this case, since our test statistic `r round(glrt, digits = 1)` is higher than the 95% quantile of the corresponding distribution ($\chi^2_{1,0.95}$).

Now, under $H_0:\,\lambda^*_2=5$, we code expressions for the log density and run optim over the starting $\lambda_2$ values from **L0**.

```{r, cache=TRUE}
# Use same reparametrisation of lambda1 again
log_dens_q6_p2 <- expression(
  + atan(theta1) * y 
  + 5 * log(cos(atan(theta1)))
  + log(2) * 3
  + 2 * lgamma(5 / 2)
  - log(pi)
  - lgamma(5)
)

deriv_q6_p2 <- deriv(log_dens_q6_p2, c("theta1"), function.arg = c("theta1", "y"))

nll_fn_q6_p2 <- function(theta1, y, N = 10000) {
  inside_sum <- NA
  
  for(j in 0:N) {
    # can compute this directly
    inside_sum[j + 1] <- sum(- log(1 + (y / (5 +  2 * j)) ^ 2))
  }
  
  g_sum <- sum(inside_sum)
  
  res <- deriv_q6_p2(theta1 = theta1, y = y)
  
  fn <- sum(as.numeric(res)) + g_sum
  
  - fn
}

# notice that the product is not here since this is constant and hence differentiates to 0
grad_fn_q6_p2 <- function(theta1, y, N = 10000) {
  res <- deriv_q6_p2(theta1 = theta1, y = y)
  
  grad <- apply(attr(res,"gradient"), 2, sum)
  
  - grad
}

fit_q6_p2 <- pick_optim(fn = nll_fn_q6_p2, gr = grad_fn_q6_p2, starting_vals = matrix(tan(L0$lambda1)), N = N_val, y = y_sample_q1)

glrt <- 2 * (-fit_q2$value + fit_q6_p2$value)
glrt > qchisq(0.95, 1)
```

So we reject $H_0$ in this case, again since our test statistic `r round(glrt, digits = 1)` is higher than $\chi^2_{1,0.95}$.

## Question 7 [10 marks]

Consider the following data frame

```{r, cache=TRUE}
#| code-fold: show
data_q7 <- read.table("http://people.bath.ac.uk/kai21/ASI/CW_2023/data_q7.txt")
```

that contains a bivariate sample 
$$(x_1,y_1),\,(x_2,y_2),\,\ldots,\,(x_n,y_n)$$ of size $n=300$.

Use the parametric family $\mathcal F_1$ defined in Question 1 to find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$, that is $f_*(y|x)$. The model should be defined by specifying the mean function $\mu(\boldsymbol{\theta}^{(1)},x)$ as follows:

$$\mu(\boldsymbol{\theta}^{(1)},x) =g^{-1}(\theta_1+\theta_2\,x +\theta_3\,x^2+\theta_4\,x^3 +\cdots+\theta_{p+1}\,x^p)$$

for some choice of link function $g$ and some choice of integer $p\geq 1$.

From a set of candidate models (that is for different choices of $g$ and $p$), choose the model with the smallest AIC (Akaike Information Criterion). Only present the results from the maximum likelihood estimation from the best chosen model and simply comment on the other models considered.

Now, repeat the same process above to find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$ but now based on the Gamma parametric family:

$$\mathcal F_{gamma}=\left\{f(y|\lambda_1,\lambda_2)=\frac{\lambda_2^{\lambda_1}}{\Gamma(\lambda_1)}y^{\lambda_1-1}\exp(-\lambda_2\,y)\,:\, \lambda_1>0\,,\lambda_2>0,y>0\right\}$$

Finally, find an appropriate model for the unknown conditional distribution of $\mathcal Y$ given $\mathcal X=x$ but now based on the Normal parametric family:
$$\mathcal F_{normal}=\left\{f(y|\lambda_1,\lambda_2)=\frac{1}{\lambda_2\sqrt{2\pi}}\,\exp\left(-\frac{(y-\lambda_1)^2}{2\lambda_2^2}\right)\,:\, \lambda_1\in {\mathcal R},\,\lambda_2>0,y\in {\mathcal R}\right\}$$

For each of the three chosen models, you should plot the data together with the maximum likelihood estimate of the mean function as well as corresponding asymptotic 95% confidence bands in the range $x\in(-3,3)$. Comment on the differences between the confidence bands and the mean function estimates. You must select the best model out of the three, based on the Akaike Information Criterion.

## Solution to Question 7

We first plot the data:

```{r}
par(mfrow = c(1,1))

plot(y ~ x, data = data_q7, pch = 20)
```

Noticing the data is in 3 distinct groups of x values, it is likely that the maximum value p can take is 2 since any higher would indicate a cubic fit which isn't possible through the 3 points.

We load the library rlang, which will help us write expressions as functions of g and p.

```{r}
library(rlang)
```

To model the dependence on $\mathcal X$ we want to fit reparametrise the model in terms of $\mu=\lambda_2\tan\lambda_1$. Then we get
$$f(y|\boldsymbol\lambda)=e^{\lambda_2 \log\cos(\tan^{-1}(\frac{\mu}{\lambda_1}))}e^{\tan^{-1}(\frac{\mu}{\lambda_1})y}g(y|\lambda_2)$$

```{r, fig.height = 10, cache=TRUE}
fit_f1_model <- function(p = 2, link = 'identity', y = data_q7$y, x = data_q7$x, N = N_val, n_starts = 100) {
  mu_expr <- expr(a)
  
  if(p > 0) {
    for(i in 1:p) mu_expr <- expr(!!mu_expr + !!sym(letters[i + 1]) * x ^ !!as.numeric(i))
  }
  
  if(link == "identity") {
    mu_expr <- mu_expr
  } else if(link == "log") {
    mu_expr <- expr(exp(!!mu_expr))
  } else if(link == "inverse") {
    mu_expr <- expr(1 / (!!mu_expr))
  } else if(link == "sin") {
    mu_expr <- expr(sin(!!mu_expr))
  } else {
    stop("Try a different link function")
  }
  
  # theta2 = log(lambda2) as before
  log_dens_outside_sum_expr <- expr(
    + y * atan(!!mu_expr / exp(theta2))
    + exp(theta2) * log(cos(atan(!!mu_expr / exp(theta2))))
    + log(2) * (exp(theta2) - 2)
    + 2 * lgamma(exp(theta2) / 2)
    - log(pi)
    - lgamma(exp(theta2))
  )
  
  log_dens_inside_sum_expr <- expr(
    - log(1 + (y / (exp(theta2) +  2 * j)) ^ 2)
  )
  
  inside_sum_deriv <- deriv(log_dens_inside_sum_expr, c("theta2"), function.arg = c("theta2", "y", "j"))
  
  if(p == 2) {
    outside_sum_deriv <- deriv(log_dens_outside_sum_expr, c("a", "b", "c", "theta2"), function.arg = c("a", "b", "c", "theta2", "y", "x"))
  } else if(p == 1) {
    outside_sum_deriv <- deriv(log_dens_outside_sum_expr, c("a", "b", "theta2"), function.arg = c("a", "b", "theta2", "y", "x"))
  } else {
    outside_sum_deriv <- deriv(log_dens_outside_sum_expr, c("a", "theta2"), function.arg = c("a", "theta2", "y", "x"))
  }
  
  eval_deriv <- function(theta, y, x) {
    if(length(theta) == 4) {
      res <- outside_sum_deriv(
        a = theta[1], 
        b = theta[2], 
        c = theta[3], 
        theta2 = theta[4], 
        y = y,
        x = x
      )
    } else if(length(theta) == 3) {
      res <- outside_sum_deriv(
        a = theta[1], 
        b = theta[2], 
        theta2 = theta[3], 
        y = y,
        x = x
      )
    } else {
      res <- outside_sum_deriv(
        a = theta[1], 
        theta2 = theta[2], 
        y = y,
        x = x
      )
    }
    
    res
  }
  
  f1_nll_fn <- function(theta, y, x, N) {
    inside_sum <- NA
    
    for(j in 0:N) {
      res <- deriv_inside_sum(theta2 = theta[length(theta)], y = y, j)
      inside_sum[j + 1] <- sum(as.numeric(res))
    }
    
    g_sum <- sum(inside_sum)
    
    res <- eval_deriv(theta, y, x)
    
    fn <- sum(as.numeric(res)) + g_sum
    
    - fn
  }
   
  f1_grad_fn <- function(theta, y, x, N) {
    inside_sum <- NA
  
    for(j in 0:N) {
      res <- inside_sum_deriv(theta2 = theta[length(theta)], y = y, j)
      inside_sum[j + 1] <- sum(attr(res,"gradient"))
    }
    
    g_sum <- sum(inside_sum, na.rm = T)
    
    res <- eval_deriv(theta, y, x)
    
    grad <- apply(attr(res, 'gradient'), 2, sum)
    
    grad[length(grad)] <- grad[length(grad)] + g_sum
    
    - grad
  }
  
  starting_vals <- matrix(NA, nrow = n_starts, ncol = p + 2)
  
  for(i in 1:n_starts) {
    starting_vals[i, ] <- rnorm(p + 2)
  }
  
  xx <- seq(-3, 3, 0.01)
  
  fit_f1 <- tryCatch(
    pick_optim(fn = f1_nll_fn, gr = f1_grad_fn, starting_vals, y = y, x = x, N = N),
    error = function(e) {
      print(e)
      
      list(
        model = list(value = NULL, hessian = diag(length(starting_vals[i, ])), convergence = 0),
        pred = vector(mode = 'integer', length = length(xx)), 
        up = vector(mode = 'integer', length = length(xx)), 
        low = vector(mode = 'integer', length = length(xx)),
        d = p + 2,
        title = paste0("No model fit")
      )
    }
  )
  
  if(!is.null(fit_f1$title)) return(fit_f1)
  
  lin_pred <- NULL
  up <- NULL
  low <- NULL
  
  for(i in 1:length(xx)) {
    x_vec <- 1
    if(p > 0) {
      for(j in 1:p) x_vec <- append(x_vec, xx[i] ^ j)
    }
    
    lin_pred[i] <- as.numeric(crossprod(fit_f1$par[c(1:(p+1))], x_vec))
    
    se <- sqrt(crossprod(x_vec, solve(fit_f1$hessian)[1:(p+1), 1:(p+1)]) %*% x_vec)
    
    up[i] <- lin_pred[i] + 1.96 * se
    low[i] <- lin_pred[i] - 1.96 * se
  }
  
  if(link == "identity") {
    lin_pred <- lin_pred
    up <- up
    low <- low
  } else if(link == "log") {
    lin_pred <- exp(lin_pred)
    up <- exp(up)
    low <- exp(low)
  } else if(link == "inverse") {
    lin_pred <- 1 / lin_pred
    up <- 1 / up
    low <- 1 / low
  }

  K <- matrix(0, p + 2, p + 2)
  for (i in 1:length(y)) {
    g <- f1_grad_fn(fit_f1$par, y[i], x[i], N)
    K <- K + g %*% t(g)
  }
  K <- K / length(y)
  
  list(
    model = fit_f1,
    pred = lin_pred, 
    up = up, 
    low = low,
    d = p + 2,
    K = K,
    title = paste0("p = ", p, ", link = ", link)
  )
}

par(mfrow = c(3, 3))

links <- c("identity", "inverse", "log")

fit_f1_q7 <- list()

for(i in 1:length(links)) {
  for(j in 1:3) {
    fit_f1_q7 <- append(fit_f1_q7, list(fit_f1_model(p = j - 1, link = links[i], N = 10000)))
  }
}

xx <- seq(-3, 3, 0.01)

for(i in 1:length(fit_f1_q7)) {
  plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
  
  pred <- fit_f1_q7[[i]]$pred
  up <- fit_f1_q7[[i]]$up
  low <- fit_f1_q7[[i]]$low
  
  lines(xx, pred, col = 'red') 
  lines(xx, up, col = 'red') 
  lines(xx, low, col = 'red')
  
  title(main = fit_f1_q7[[i]]$title)
}
```

We then calculate AIC for each model and pick the one with the lowest:

```{r}
# Take out p = 1,2 for inverse since something has clearly gone wrong from graph
fit_f1_q7 <- fit_f1_q7[-c(5, 6)]

calc_aic <- function(model, d) 2 * (model$value + d)

min_aic_ind <- which.min(sapply(fit_f1_q7, \(x) calc_aic(x$model, x$d)))

best_f1_fit <- fit_f1_q7[[min_aic_ind]]

best_f1_fit$title
```

So the best fitting model we considered is $\mu(\boldsymbol\theta^{(1)},x)=\theta_1+\theta_2x+\theta_3x^2$. We plot results from that below:

```{r, fig.height=7}
par(mfrow = c(1, 1))

pred <- best_f1_fit$pred
up <- best_f1_fit$up
low <- best_f1_fit$low

plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
lines(xx, up, col = 'red') 
lines(xx, pred, col = 'red') 
lines(xx, low, col = 'red')
title(main = "F1 approximation to the data")
```

For $\mathcal F_{gamma}$, notice that
$$f(y|\lambda_1,\lambda_2)=\exp(-\lambda_2y+\lambda_1\log\lambda_2-\log\Gamma(\lambda_1)+(\lambda_1 - 1)\log y)$$
And we know that $\mathbb E[Y] = \frac{1}{\mu}$ by the properties of the Gamma distribution, so we reparametrise $\mu=\frac{\lambda_1}{\lambda_2}$, to get the following:
$$\exp(-\frac{\lambda_1}{\mu}y-\lambda_1\log\mu+\lambda_1\log\lambda_1-\log\Gamma(\lambda_1)+(\lambda_1-1)\log y)$$

```{r, fig.height = 10, cache=TRUE}
fit_gamma_model <- function(p = 2, link = 'inverse', y = data_q7$y, x = data_q7$x, n_starts = 100) {
  mu_expr <- expr(a)
  
  if(p > 0) {
    for(i in 1:p) mu_expr <- expr(!!mu_expr + !!sym(letters[i + 1]) * x ^ !!as.numeric(i))
  }
  
  if(link == "identity") {
    mu_expr <- mu_expr
  } else if(link == "log") {
    mu_expr <- expr(exp(!!mu_expr))
  } else if(link == "inverse") {
    mu_expr <- expr(1 / (!!mu_expr))
  } else if(link == "sin") {
    mu_expr <- expr(asin(!!mu_expr))
  } else {
    stop("Try a different link function")
  }
  
  # theta1 = log(lambda1)
  log_dens_gamma_expr <- expr(
    - (y * exp(theta1)) / !!mu_expr
    - exp(theta1) * log(!!mu_expr)
    + theta1 * exp(theta1)
    + (exp(theta1) - 1) * log(y)
    - lgamma(exp(theta1))
  )
  
  if(p == 2) {
    gamma_deriv <- deriv(log_dens_gamma_expr, c("a", "b", "c", "theta1"), function.arg = c("a", "b", "c", "theta1", "y", "x"))
  } else if(p == 1) {
    gamma_deriv <- deriv(log_dens_gamma_expr, c("a", "b", "theta1"), function.arg = c("a", "b", "theta1", "y", "x"))
  } else {
    gamma_deriv <- deriv(log_dens_gamma_expr, c("a", "theta1"), function.arg = c("a", "theta1", "y", "x"))
  }
  
  eval_deriv <- function(theta, y, x) {
    if(length(theta) == 4) {
      res <- gamma_deriv(
        a = theta[1], 
        b = theta[2], 
        c = theta[3], 
        theta1 = theta[4], 
        y = y,
        x = x
      )
    } else if(length(theta) == 3) {
      res <- gamma_deriv(
        a = theta[1], 
        b = theta[2], 
        theta1 = theta[3], 
        y = y,
        x = x
      )
    } else {
      res <- gamma_deriv(
        a = theta[1], 
        theta1 = theta[2], 
        y = y,
        x = x
      )
    }
  }
  
  gamma_nll_fn <- function(theta, y, x) {
    res <- eval_deriv(theta, y, x)
    
    fn <- sum(as.numeric(res))
    
    - fn
  }
   
  gamma_grad_fn <- function(theta, y, x) {
    res <- eval_deriv(theta, y, x)
    
    grad <- apply(attr(res, 'gradient'), 2, sum)
    
    - grad
  }
  
  starting_vals <- matrix(NA, nrow = n_starts, ncol = p + 2)
  
  for(i in 1:n_starts) {
    starting_vals[i, ] <- runif(p + 2, 0, 10)
  }
  
  xx <- seq(-3, 3, 0.01)
  
  fit_gamma <- tryCatch(
    pick_optim(fn = gamma_nll_fn, gr = gamma_grad_fn, starting_vals, y = y, x = x),
    error = function(e) {
      print(e)
      
      list(
        model = list(value = NULL, hessian = diag(length(starting_vals[i, ])), convergence = 0),
        pred = vector(mode = 'integer', length = length(xx)), 
        up = vector(mode = 'integer', length = length(xx)), 
        low = vector(mode = 'integer', length = length(xx)),
        d = p + 2,
        title = paste0("No model fit")
      )
    }
  )
  
  if(!is.null(fit_gamma$title)) return(fit_gamma)
  
  lin_pred <- NULL
  up <- NULL
  low <- NULL
  
  for(i in 1:length(xx)) {
    x_vec <- 1
    if(p > 0) {
      for(j in 1:p) x_vec <- append(x_vec, xx[i] ^ j)
    }
    
    lin_pred[i] <- as.numeric(crossprod(fit_gamma$par[c(1:(p+1))], x_vec))
    
    se <- sqrt(crossprod(x_vec, solve(fit_gamma$hessian)[1:(p+1), 1:(p+1)]) %*% x_vec)
    
    up[i] <- lin_pred[i] + 1.96 * se
    low[i] <- lin_pred[i] - 1.96 * se
  }
  
  if(link == "identity") {
    lin_pred <- lin_pred
    up <- up
    low <- low
  } else if(link == "log") {
    lin_pred <- exp(lin_pred)
    up <- exp(up)
    low <- exp(low)
  } else if(link == "inverse") {
    lin_pred <- 1 / lin_pred
    up <- 1 / up
    low <- 1 / low
  } else if(link == "sin") {
    lin_pred <- asin(lin_pred)
    up <- asin(up)
    low <- asin(low)
  }

  K <- matrix(0, p + 2, p + 2)
  for (i in 1:length(y)) {
    g <- gamma_grad_fn(fit_gamma$par, y[i], x[i])
    K <- K + g %*% t(g)
  }
  K <- K / length(y)
  
  list(
    model = fit_gamma,
    pred = lin_pred, 
    up = up, 
    low = low,
    d = p + 2,
    K = K,
    title = paste0("p = ", p, ", link = ", link)
  )
}

par(mfrow = c(3, 3))

links <- c("identity", "inverse", "log")

fit_gamma_q7 <- list()

for(i in 1:length(links)) {
  for(j in 1:3) {
    fit_gamma_q7 <- append(fit_gamma_q7, list(fit_gamma_model(p = j - 1, link = links[i])))
  }
}

xx <- seq(-3, 3, 0.01)

for(i in 1:length(fit_gamma_q7)) {
  plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
  
  pred <- fit_gamma_q7[[i]]$pred
  up <- fit_gamma_q7[[i]]$up
  low <- fit_gamma_q7[[i]]$low
  
  lines(xx, pred, col = 'red') 
  lines(xx, up, col = 'red') 
  lines(xx, low, col = 'red')
  
  title(main = fit_gamma_q7[[i]]$title)
}
```

We can compare these models with AIC and pick the best ranking model:

```{r}
min_aic_ind <- which.min(sapply(fit_gamma_q7, \(x) calc_aic(x$model, x$d)))

best_gamma_fit <- fit_gamma_q7[[min_aic_ind]]

best_gamma_fit$title
```

So our best model in the Gamma case out of the ones we considered is $\mu(\boldsymbol\theta^{(1)},x)=\theta_1+\theta_2x+\theta_3x^2$. The results from this model are plotted below:

```{r, fig.height=7}
par(mfrow = c(1, 1))

pred <- best_gamma_fit$pred
up <- best_gamma_fit$up
low <- best_gamma_fit$low

plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
lines(xx, up, col = 'red') 
lines(xx, pred, col = 'red') 
lines(xx, low, col = 'red')
title(main = "Gamma approximation to the data")
```


Finally, for $\mathcal F_{normal}$, we can rearrange the expression for the density so that
$$f(y|\lambda_1,\lambda_2)=\exp(\frac{\lambda_1y-\frac{\lambda_1^2}{2}}{\lambda_2^2}-\frac{y^2}{2\lambda_2^2}-\frac{\log(2\pi\lambda_2^2)}{2})$$

```{r, fig.height = 10, cache=TRUE}
fit_norm_model <- function(p = 2, link = 'identity', y = data_q7$y, x = data_q7$x, n_starts = 100) {
  lambda_expr <- expr(a)
  
  if(p > 0) {
    for(i in 1:p) lambda_expr <- expr(!!lambda_expr + !!sym(letters[i + 1]) * x ^ !!as.numeric(i))
  }
  
  if(link == "identity") {
    lambda_expr <- lambda_expr
  } else if(link == "log") {
    lambda_expr <- expr(exp(!!lambda_expr))
  } else if(link == "inverse") {
    lambda_expr <- expr(1/(!!lambda_expr))
  } else {
    stop("Try a different link function")
  }
  
  # reparam with lambda2 = exp(theta2)
  log_dens_norm_expr <- rlang::expr(
    + !!lambda_expr * y / exp(theta2) ^ 2
    - (!!lambda_expr) ^ 2 / (2 * exp(theta2) ^ 2)
    - y ^ 2 / (2 * exp(theta2) ^ 2)
    - log(2 * pi * exp(theta2) ^ 2) / 2
  )
  
  if(p == 2) {
    norm_deriv <- deriv(log_dens_norm_expr, c("a", "b", "c", "theta2"), function.arg = c("a", "b", "c", "theta2", "y", "x"))
  } else if(p == 1) {
    norm_deriv <- deriv(log_dens_norm_expr, c("a", "b", "theta2"), function.arg = c("a", "b", "theta2", "y", "x"))
  } else {
    norm_deriv <- deriv(log_dens_norm_expr, c("a", "theta2"), function.arg = c("a", "theta2", "y", "x"))
  }
  
  eval_deriv <- function(theta, y, x) {
    if(length(theta) == 4) {
      res <- norm_deriv(
        a = theta[1], 
        b = theta[2], 
        c = theta[3], 
        theta2 = theta[4], 
        y = y,
        x = x
      )
    } else if(length(theta) == 3) {
      res <- norm_deriv(
        a = theta[1], 
        b = theta[2], 
        theta2 = theta[3], 
        y = y,
        x = x
      )
    } else {
      res <- norm_deriv(
        a = theta[1], 
        theta2 = theta[2], 
        y = y,
        x = x
      )
    }
  }
  
  norm_nll_fn <- function(theta, y, x) {
    res <- eval_deriv(theta, y, x)
    
    fn <- sum(as.numeric(res))
    
    - fn
  }
   
  norm_grad_fn <- function(theta, y, x) {
    res <- eval_deriv(theta, y, x)
    
    grad <- apply(attr(res, 'gradient'), 2, sum)
    
    - grad
  }
  
  starting_vals <- matrix(NA, nrow = n_starts, ncol = p + 2)
  
  for(i in 1:n_starts) {
    starting_vals[i, ] <- rnorm(p + 2)
  }
  
  fit_norm <- pick_optim(fn = norm_nll_fn, gr = norm_grad_fn, starting_vals, y = y, x = x)
  
  lin_pred <- NULL
  up <- NULL
  low <- NULL
  
  xx <- seq(-3, 3, 0.01)
  
  for(i in 1:length(xx)) {
    x_vec <- 1
    if(p > 0) {
      for(j in 1:p) x_vec <- append(x_vec, xx[i] ^ j)
    }
    
    lin_pred[i] <- as.numeric(crossprod(fit_norm$par[c(1:(p+1))], x_vec))
    
    se <- sqrt(crossprod(x_vec, solve(fit_norm$hessian)[1:(p+1), 1:(p+1)]) %*% x_vec)
    
    up[i] <- lin_pred[i] + 1.96 * se
    low[i] <- lin_pred[i] - 1.96 * se
  }
  
  if(link == "identity") {
    lin_pred <- lin_pred
    up <- up
    low <- low
  } else if(link == "log") {
    lin_pred <- exp(lin_pred)
    up <- exp(up)
    low <- exp(low)
  } else if(link == "inverse") {
    lin_pred <- 1 / lin_pred
    up <- 1 / up
    low <- 1 / low
  }
  
  K <- matrix(0, p + 2, p + 2)
  for (i in 1:length(y)) {
    g <- norm_grad_fn(fit_norm$par, y[i], x[i])
    K <- K + g %*% t(g)
  }
  K <- K / length(y)
  
  list(
    model = fit_norm,
    pred = lin_pred, 
    up = up, 
    low = low,
    d = p + 2,
    K = K,
    title = paste0("p = ", p, ", link = ", link)
  )
}

par(mfrow = c(3, 3))

links <- c("identity", "inverse", "log")

fit_norm_q7 <- NULL

for(i in 1:length(links)) {
  for(j in 1:3) {
    fit_norm_q7 <- append(fit_norm_q7, list(fit_norm_model(p = j - 1, link = links[i])))
  }
}

xx <- seq(-3, 3, 0.01)

for(i in 1:length(fit_norm_q7)) {
  plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
  
  pred <- fit_norm_q7[[i]]$pred
  up <- fit_norm_q7[[i]]$up
  low <- fit_norm_q7[[i]]$low
  
  lines(xx, pred, col = 'red') 
  lines(xx, up, col = 'red') 
  lines(xx, low, col = 'red')
  
  title(main = fit_norm_q7[[i]]$title)
}
```

As before, we can select the model with the lowest AIC:

```{r}
min_aic_ind <- which.min(sapply(fit_norm_q7, \(x) calc_aic(x$model, x$d)))

best_norm_fit <- fit_norm_q7[[min_aic_ind]]

best_norm_fit$title
```

That is, the model $\mu(\boldsymbol\theta^{(1)},x)=\theta_1+\theta_2x+\theta_3x^2$. We plot this below:

```{r, fig.height=7}
par(mfrow = c(1, 1))

pred <- best_norm_fit$pred
up <- best_norm_fit$up
low <- best_norm_fit$low

plot(y ~ x, data = data_q7, pch = 20, xlim = c(-3, 3))
lines(xx, pred, col = 'red') 
lines(xx, up, col = 'red') 
lines(xx, low, col = 'red')
title(main = "Normal approximation to the data")
```

All three models actually make very similar estimations of the mean function. This is likely since they all use the same link function, so potentially indicates that we should try a wider range of link functions if we ran the code again. The confidence intervals all follow a similar pattern, where they are closer around the clumps of data, but widen away from the data. Out of the 3, the Normal distribution seems to have the smallest confidence intervals and Gamma the biggest, although the $F_1$ confidence intervals are very close to the Gamma approximation.

We calculate AIC for all 3 models and select the lowest:

```{r}
norm_aic <- calc_aic(best_norm_fit$model, best_norm_fit$d)
gamma_aic <- calc_aic(best_gamma_fit$model, best_gamma_fit$d)
f1_aic <- calc_aic(best_f1_fit$model, best_f1_fit$d)

norm_aic
gamma_aic
f1_aic
```

That is, the distribution defined in Q1.

## Question 8 [4 marks]

Use the data in Question 7 to compute 95% confidence intervals for the least worse value of the mean function at each $x$, that is $\mu(\boldsymbol{\theta}^{(1)}_\dagger,x)$ for each of the three parametric families: $\mathcal F_1$, the Gamma and the Normal. Plot the computed confidence bands in the range $x\in(-3,3)$ for each parametric family and comment on the differences obtained.

## Solution to Question 8

We use results from lectures notes that
$$\left\{\left[\widehat{\boldsymbol{\mathcal J} (\boldsymbol{\theta}^\dagger})\right]^{-1}\widehat{\boldsymbol{\mathcal K} (\boldsymbol{\theta}^\dagger)}\left[\widehat{\boldsymbol{\mathcal J} (\boldsymbol{\theta}^\dagger})\right]^{-1}\right\}^{-1/2}(\widehat{\boldsymbol{\theta}}_n(\boldsymbol{\mathcal Y})-\boldsymbol{\theta}^\dagger)\stackrel{d}{\to} N(\boldsymbol{0}_{p+m},\boldsymbol{I}_{p+m})\quad \mbox{as} \quad n\to \infty$$
where $\widehat{\boldsymbol{\mathcal J} (\boldsymbol{\theta}^\dagger)}$ and $\widehat{\boldsymbol{\mathcal K} (\boldsymbol{\theta}^\dagger)}$ were defined in lectures.

Using this result, we can calculate asymptotic confidence intervals for $\boldsymbol{\theta}^\dagger$:

```{r, cache=TRUE}
calculate_dagger_ci <- function(fit) {
  J <- fit$model$hessian
  asymp_var <- solve(J) %*% fit$K %*% solve(J)
  
  CI <- matrix(NA, ncol = 2, nrow = length(fit$model$par) - 1)
  for (i in (1:length(fit$model$par) - 1)) {
    CI[i, ] <- fit$model$par[i] + c(-1, 1) * 1.96 * sqrt(asymp_var[i, i])
  }
  
  CI
}

fits <- list(
  Normal = best_norm_fit,
  Gamma = best_gamma_fit,
  F1 = best_f1_fit
)

cis <- lapply(fits, calculate_dagger_ci)
```

We can use these confidence intervals to calculate confidence intervals for $\mu(\boldsymbol{\theta}^{(1)}_\dagger,x)$:

```{r, fig.height=10}
par(mfrow = c(3,1))

xx <- seq(-3, 3, by = 0.01)

distributions <- c("Normal", "Gamma", "F1")

for(d in distributions) {
  low <- cis[[d]][1, 1] + cis[[d]][2, 1] * xx + cis[[d]][3, 1] * xx ^ 2
  high <- cis[[d]][1, 2] + cis[[d]][2, 2] * xx + cis[[d]][3, 2] * xx ^ 2
  
  plot(xx, low, type = "l", ylim = c(8, 22), xlab = "x", ylab = expression(mu))
  lines(xx, high, type = "l")
  title(main = d)
}
```

